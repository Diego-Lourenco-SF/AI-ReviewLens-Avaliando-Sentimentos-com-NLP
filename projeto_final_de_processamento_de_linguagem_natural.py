# -*- coding: utf-8 -*-
"""Projeto Final de Processamento de Linguagem Natural.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ss1E7524-v0YRIml7IlkTXIPteLBDsIh
"""

# Importar bibliotecas necessárias
import pandas as pd
import numpy as np
import re
import nltk
from nltk.corpus import stopwords
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.svm import SVC
from sklearn.metrics import classification_report
from gensim.models import KeyedVectors
from transformers import BertTokenizerFast, BertForSequenceClassification, AdamW
from torch.utils.data import DataLoader, Dataset
from sklearn.preprocessing import StandardScaler
import torch
import zipfile

# Baixar recursos necessários do NLTK
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('punkt_tab')

# Caminho do arquivo zip contendo o dataset
file_path = "/content/drive/MyDrive/Colab Notebooks/Projeto Final De PNL 2024/Reviews.csv.zip"

# Carregar o dataset
print("Carregando os dados...")
df = pd.read_csv(file_path, compression='zip')
print("Dados carregados com sucesso!")

# Exibir as primeiras linhas do dataset com layout bonito
print("\n===== Visualizando os dados =====")
print(df.head().to_markdown(index=False))
print("\n=================================")

# Selecionar colunas de interesse: "Score" e "Text"
print("Selecionando colunas relevantes...")
df = df[["Score", "Text"]].dropna()

# Converter "Score" para categorias de sentimento (positivo, neutro, negativo)
def map_score_to_sentiment(score):
    if score >= 4:
        return 'positivo'
    elif score == 3:
        return 'neutro'
    else:
        return 'negativo'

df['Sentiment'] = df['Score'].apply(map_score_to_sentiment)

# Exibir distribuição dos sentimentos
print("Distribuição dos sentimentos:")
print(df['Sentiment'].value_counts())

# Função de pré-processamento do texto
def preprocess_text(text):
    text = re.sub(r'\d+', '', text)  # Remover números
    text = re.sub(r'[^\w\s]', '', text)  # Remover pontuação
    text = text.lower()  # Converter para minúsculas
    tokens = nltk.word_tokenize(text)  # Tokenizar
    stop_words = set(stopwords.words('english'))  # Stopwords em inglês
    tokens = [word for word in tokens if word not in stop_words]  # Remover stopwords
    return ' '.join(tokens)

# Aplicar pré-processamento ao texto das avaliações
print("Realizando pré-processamento dos textos...")
df['Cleaned_Text'] = df['Text'].apply(preprocess_text)

# Dividir os dados em conjuntos de treinamento, validação e teste
print("Dividindo os dados em treinamento, validação e teste...")
X = df['Cleaned_Text']
y = df['Sentiment']

# Primeira divisão: treino
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

# Segunda divisão: validação e teste
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)

# Transformar textos em representacoes Bag-of-Words
print("Convertendo textos para representacoes Bag-of-Words...")
vectorizer = CountVectorizer()
X_train_bow = vectorizer.fit_transform(X_train)
X_val_bow = vectorizer.transform(X_val)
X_test_bow = vectorizer.transform(X_test)

# Escalar os dados
scaler = StandardScaler(with_mean=False)
X_train_bow = scaler.fit_transform(X_train_bow)
X_val_bow = scaler.transform(X_val_bow)
X_test_bow = scaler.transform(X_test_bow)

# Modelo 1: Treinar classificador SVM com Bag-of-Words
print("Treinando o modelo SVM com Bag-of-Words...")
svm_bow = SVC(kernel='linear', random_state=42, max_iter=5000)
svm_bow.fit(X_train_bow[:7000], y_train[:7000])

# Fazer previsões no conjunto de validação para o modelo 1
y_val_pred = svm_bow.predict(X_val_bow)

# Avaliar o desempenho no conjunto de validação para o modelo 1
print("Avaliando o desempenho no conjunto de validação para o modelo SVM com Bag-of-Words...")
print(classification_report(y_val, y_val_pred))

# Fazer previsões no conjunto de teste para o modelo 1
y_test_pred = svm_bow.predict(X_test_bow)

# Avaliar o desempenho no conjunto de teste para o modelo 1
print("Avaliando o desempenho no conjunto de teste para o modelo SVM com Bag-of-Words...")
print(classification_report(y_test, y_test_pred))

# Modelo 2: SVM com Embeddings
print("Carregando embeddings pré-treinados...")

# Caminho do arquivo compactado
embeddings_zip_path = '/content/drive/MyDrive/Colab Notebooks/Projeto Final De PNL 2024/w2v-ptbr-skips50.zip'
embeddings_extracted_path = '/content/drive/MyDrive/Colab Notebooks/Projeto Final De PNL 2024/skip_s50.txt'

# Extrair o arquivo
print("Extraindo embeddings pré-treinados...")
with zipfile.ZipFile(embeddings_zip_path, 'r') as zip_ref:
    zip_ref.extractall('/content/drive/MyDrive/Colab Notebooks/Projeto Final De PNL 2024/')

# Carregar embeddings descompactados
embeddings_path = embeddings_extracted_path
word_vectors = KeyedVectors.load_word2vec_format(embeddings_path, binary=False)
embedding_dim = word_vectors.vector_size


def get_average_embedding(text):
    tokens = text.split()
    embeddings = [word_vectors[word] for word in tokens if word in word_vectors]
    return np.mean(embeddings, axis=0) if embeddings else np.zeros(embedding_dim)

print("Calculando embeddings para treinamento, validação e teste...")
X_train_embed = np.array([get_average_embedding(text) for text in X_train[:7000]])  # Usar 7 mil amostras para treinamento
X_val_embed = np.array([get_average_embedding(text) for text in X_val])
X_test_embed = np.array([get_average_embedding(text) for text in X_test])

print("Treinando o modelo SVM com Embeddings...")
svm_embed = SVC(kernel='linear', random_state=42)
svm_embed.fit(X_train_embed, y_train[:7000])  # Usar os mesmos 7 mil rótulos

print("Avaliando o modelo SVM com Embeddings...")
y_val_pred_embed = svm_embed.predict(X_val_embed)
print(classification_report(y_val, y_val_pred_embed))

y_test_pred_embed = svm_embed.predict(X_test_embed)
print(classification_report(y_test, y_test_pred_embed))

# Modelo 3: BERT
print("Preparando dados para BERT...")
bert_tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')

def encode_texts(texts, tokenizer, max_length=128):
    return tokenizer(list(texts), truncation=True, padding=True, max_length=max_length, return_tensors='pt')

train_encodings = encode_texts(X_train[:7000], bert_tokenizer)
val_encodings = encode_texts(X_val, bert_tokenizer)
test_encodings = encode_texts(X_test, bert_tokenizer)

class SentimentDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        item = {key: val[idx] for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

label_map = {'positivo': 0, 'neutro': 1, 'negativo': 2}
y_train_encoded = [label_map[label] for label in y_train[:7000]]
y_val_encoded = [label_map[label] for label in y_val]
y_test_encoded = [label_map[label] for label in y_test]

train_dataset = SentimentDataset(train_encodings, y_train_encoded)
val_dataset = SentimentDataset(val_encodings, y_val_encoded)
test_dataset = SentimentDataset(test_encodings, y_test_encoded)

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=16)
test_loader = DataLoader(test_dataset, batch_size=16)

print("Carregando modelo BERT...")
bert_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
bert_model.to(device)

optimizer = AdamW(bert_model.parameters(), lr=5e-5)

print("Treinando modelo BERT...")
bert_model.train()
gradient_accumulation_steps = 4  # Gradientes acumulados
for epoch in range(1):  # Reduzido para 1 época
    for idx, batch in enumerate(train_loader):
        if idx >= 437:  # Limitar a 7 mil amostras
            break
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        outputs = bert_model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss / gradient_accumulation_steps  # Normalizar perda
        loss.backward()

        # Atualizar a cada 4 lotes
        if (idx + 1) % gradient_accumulation_steps == 0:
            optimizer.step()
            optimizer.zero_grad()

print("Avaliando modelo BERT...")
bert_model.eval()
preds = []
true_labels = []
with torch.no_grad():
    for batch in test_loader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)
        outputs = bert_model(input_ids, attention_mask=attention_mask)
        preds.extend(torch.argmax(outputs.logits, axis=1).cpu().numpy())
        true_labels.extend(labels.cpu().numpy())

print(classification_report(true_labels, preds, target_names=label_map.keys()))

print("Pipeline de pré-processamento e treinamento concluído!")